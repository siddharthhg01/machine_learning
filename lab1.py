# -*- coding: utf-8 -*-
"""Lab1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-_k8cB1G0EEI66OyUGTAtfp2HlLp9STp
"""



import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Load Diabetes dataset (update with your file path in Google Colab)
diabetes_df = pd.read_csv("/content/Dataset of Diabetes .csv")

# Load Adult Income dataset (update with your file path in Google Colab)
adult_income_df = pd.read_csv("/content/adult.csv")

# Function for data cleaning and transformations
def preprocess_data(df):
    # 1. Data Cleaning: Handle Missing Values

    # Identify columns with missing values
    missing_values = df.isnull().sum()
    print("Columns with missing values:\n", missing_values[missing_values > 0])

    # Handle missing values: Use median for numerical columns and most frequent for categorical columns
    numerical_cols = df.select_dtypes(include=["float64", "int64"]).columns
    categorical_cols = df.select_dtypes(include=["object"]).columns

    # Impute numerical columns with median, and categorical columns with the most frequent value
    imputer = SimpleImputer(strategy='most_frequent')
    df[categorical_cols] = imputer.fit_transform(df[categorical_cols])

    imputer = SimpleImputer(strategy='median')
    df[numerical_cols] = imputer.fit_transform(df[numerical_cols])

    # 2. Handling Categorical Data: One-Hot Encoding
    print("\nCategorical Columns:\n", categorical_cols)

    # Apply OneHotEncoder to categorical columns
    encoder = OneHotEncoder(drop='first', sparse_output=False)
    encoded_categorical_data = encoder.fit_transform(df[categorical_cols])

    # Creating a DataFrame for encoded categorical data
    encoded_categorical_df = pd.DataFrame(encoded_categorical_data, columns=encoder.get_feature_names_out(categorical_cols))

    # Drop original categorical columns and concatenate the encoded columns
    df = df.drop(categorical_cols, axis=1)
    df = pd.concat([df, encoded_categorical_df], axis=1)

    # 3. Handling Outliers: Using Z-score (Optional - can be added if needed)
    from scipy import stats
    z_scores = np.abs(stats.zscore(df[numerical_cols]))
    df = df[(z_scores < 3).all(axis=1)]  # Remove rows with outliers

    # 4. Data Transformations: Min-Max Scaling
    min_max_scaler = MinMaxScaler()
    df[numerical_cols] = min_max_scaler.fit_transform(df[numerical_cols])

    # 5. Data Transformations: Standardization
    standard_scaler = StandardScaler()
    df[numerical_cols] = standard_scaler.fit_transform(df[numerical_cols])

    return df


# Apply preprocessing for Diabetes dataset
diabetes_df = preprocess_data(diabetes_df)
print("\nPreprocessed Diabetes Dataset:")
print(diabetes_df.head())

# Apply preprocessing for Adult Income dataset
adult_income_df = preprocess_data(adult_income_df)
print("\nPreprocessed Adult Income Dataset:")
print(adult_income_df.head())